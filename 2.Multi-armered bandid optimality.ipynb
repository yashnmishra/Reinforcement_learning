{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armered bandid optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandit (MAB): Definition, Use, and Applications\n",
    "\n",
    "#### Definition\n",
    "The Multi-Armed Bandit (MAB) problem is a classic example of a reinforcement learning scenario where an agent must choose between multiple options (referred to as \"arms\"), each providing a reward that follows an unknown probability distribution. The goal is to maximize the total reward over a series of trials by balancing the exploration of new arms and the exploitation of known rewarding arms.\n",
    "\n",
    "- **Arms**: The different options or actions available to the agent.\n",
    "- **Reward**: The outcome or payoff received from selecting an arm, which can vary and is often stochastic.\n",
    "- **Exploration**: Trying different arms to gather information about their reward distributions.\n",
    "- **Exploitation**: Selecting the arm that is currently known to provide the highest reward.\n",
    "\n",
    "#### Use\n",
    "The MAB problem is used to model situations where decisions need to be made sequentially under uncertainty, with the objective of optimizing the cumulative reward. The core challenge is to find an optimal balance between exploration and exploitation.\n",
    "\n",
    "#### Applications\n",
    "The MAB framework is widely applicable in various fields, including:\n",
    "\n",
    "1. **Online Advertising**:\n",
    "    - **Problem**: An advertiser wants to determine which ads to display to users to maximize click-through rates.\n",
    "    - **Solution**: Each ad is an arm, and the reward is the click rate. The MAB algorithm selects ads to show based on past performance to maximize total clicks.\n",
    "\n",
    "2. **Clinical Trials**:\n",
    "    - **Problem**: Researchers want to find the most effective treatment among several options.\n",
    "    - **Solution**: Each treatment is an arm, and the reward is the patient’s health outcome. The MAB algorithm helps allocate patients to treatments to identify the best one while minimizing harm.\n",
    "\n",
    "3. **Recommendation Systems**:\n",
    "    - **Problem**: A recommendation system needs to suggest products or content to users.\n",
    "    - **Solution**: Each product/content is an arm, and the reward is user engagement (e.g., clicks, views). The MAB algorithm learns user preferences to provide better recommendations.\n",
    "\n",
    "4. **A/B Testing**:\n",
    "    - **Problem**: A company wants to determine which version of a webpage or feature performs better.\n",
    "    - **Solution**: Each version is an arm, and the reward is user behavior metrics (e.g., conversion rates). The MAB algorithm dynamically adjusts the exposure of different versions to identify the best one.\n",
    "\n",
    "5. **Finance**:\n",
    "    - **Problem**: An investor wants to allocate capital among different investment options.\n",
    "    - **Solution**: Each investment is an arm, and the reward is the return on investment. The MAB algorithm helps in dynamically adjusting the portfolio to maximize returns.\n",
    "\n",
    "### Example Code: Epsilon-Greedy Algorithm for MAB\n",
    "Here's an example of a simple MAB algorithm called epsilon-greedy, which balances exploration and exploitation by choosing a random arm with probability ε and the best-known arm with probability 1-ε.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)  # Number of times each arm was pulled\n",
    "        self.values = np.zeros(n_arms)  # Average reward for each arm\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)  # Explore: Random arm\n",
    "        else:\n",
    "            return np.argmax(self.values)  # Exploit: Best known arm\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        value = self.values[arm]\n",
    "        # Update the estimated value of the arm\n",
    "        self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage:\n",
    "n_arms = 3  # Number of arms\n",
    "bandit = EpsilonGreedyBandit(n_arms, epsilon=0.1)\n",
    "\n",
    "# Simulated rewards for each arm\n",
    "true_rewards = [0.2, 0.5, 0.8]\n",
    "\n",
    "# Simulate pulling arms and getting rewards\n",
    "for _ in range(1000):\n",
    "    arm = bandit.select_arm()\n",
    "    reward = np.random.binomial(1, true_rewards[arm])\n",
    "    bandit.update(arm, reward)\n",
    "\n",
    "print(\"Arm selection counts:\", bandit.counts)\n",
    "print(\"Estimated values:\", bandit.values)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **Initialization**: The number of arms and the exploration probability ε are initialized.\n",
    "- **Arm Selection**: With probability ε, a random arm is selected (exploration). Otherwise, the arm with the highest estimated reward is selected (exploitation).\n",
    "- **Reward Update**: After each arm pull, the estimated reward for that arm is updated based on the observed reward.\n",
    "\n",
    "### Conclusion\n",
    "The Multi-Armed Bandit problem is a versatile and powerful framework for optimizing decisions in uncertain environments. Its applications range from online advertising to finance, making it a critical tool in fields that require adaptive learning and optimization. By balancing exploration and exploitation, MAB algorithms can significantly enhance decision-making processes and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with respect to chess problem :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move selection counts: [14. 26. 19. 41.]\n",
      "Estimated values: [0.3738409  0.59157984 0.48419605 0.71567536]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ChessMultiArmedBandit:\n",
    "    def __init__(self, moves):\n",
    "        self.moves = moves\n",
    "        self.n_arms = len(moves)\n",
    "        self.counts = np.zeros(self.n_arms)\n",
    "        self.values = np.zeros(self.n_arms)\n",
    "\n",
    "    def select_move(self):\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts < self.n_arms:\n",
    "            return int(total_counts)\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(total_counts) / self.counts)\n",
    "        return int(np.argmax(ucb_values))\n",
    "\n",
    "    def update(self, move_index, reward):\n",
    "        self.counts[move_index] += 1\n",
    "        n = self.counts[move_index]\n",
    "        value = self.values[move_index]\n",
    "        self.values[move_index] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage:\n",
    "moves = [\"e4\", \"d4\", \"Nf3\", \"c4\"]  # Example opening moves\n",
    "bandit = ChessMultiArmedBandit(moves)\n",
    "\n",
    "# Simulated rewards for moves (these would come from a chess engine or game outcomes)\n",
    "rewards = [0.4, 0.6, 0.5, 0.7]\n",
    "\n",
    "# Simulate selecting moves and getting rewards\n",
    "for _ in range(100):\n",
    "    move_index = bandit.select_move()\n",
    "    reward = rewards[move_index] + random.normalvariate(0, 0.1)  # Add some noise\n",
    "    bandit.update(move_index, reward)\n",
    "\n",
    "print(\"Move selection counts:\", bandit.counts)\n",
    "print(\"Estimated values:\", bandit.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain bandid optimality wrt. seat allocation where no. of seats avaiable are 30 and number of students applied are 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 students' indices: [ 63  27 125  73  84 179  93  57   1  65 141 152 103 110  59  87 154  72\n",
      " 183 101  79 126  38  47  30 193   6 135 181 164]\n",
      "Top 30 students' estimated values: [0.84009787 0.84745795 0.85640666 0.85942675 0.86636437 0.86651025\n",
      " 0.8671897  0.86896978 0.87055183 0.87891523 0.88908567 0.88950325\n",
      " 0.8956814  0.90949962 0.91247572 0.91823801 0.92219421 0.92597792\n",
      " 0.92900142 0.9410149  0.9423761  0.94501673 0.9458296  0.952999\n",
      " 0.9590935  0.97237415 0.9898367  1.010532   1.02391396 1.05905692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StudentSelectionBandit:\n",
    "    def __init__(self, n_students):\n",
    "        self.n_students = n_students\n",
    "        self.counts = np.zeros(n_students)\n",
    "        self.values = np.zeros(n_students)\n",
    "\n",
    "    def select_student(self):\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts < self.n_students:\n",
    "            return int(total_counts)\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(total_counts) / self.counts)\n",
    "        return int(np.argmax(ucb_values))\n",
    "\n",
    "    def update(self, student_index, reward):\n",
    "        self.counts[student_index] += 1\n",
    "        n = self.counts[student_index]\n",
    "        value = self.values[student_index]\n",
    "        self.values[student_index] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage:\n",
    "n_students = 200\n",
    "bandit = StudentSelectionBandit(n_students)\n",
    "\n",
    "# Simulated initial rewards for students (e.g., entrance test scores)\n",
    "initial_rewards = np.random.rand(n_students)\n",
    "\n",
    "# Simulate selecting students and getting additional rewards\n",
    "for _ in range(500):  # More iterations to refine estimates\n",
    "    student_index = bandit.select_student()\n",
    "    reward = initial_rewards[student_index] + random.normalvariate(0, 0.1)  # Add some noise\n",
    "    bandit.update(student_index, reward)\n",
    "\n",
    "# Select the top 30 students based on the refined values\n",
    "top_students = np.argsort(bandit.values)[-30:]\n",
    "\n",
    "print(\"Top 30 students' indices:\", top_students)\n",
    "print(\"Top 30 students' estimated values:\", bandit.values[top_students])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
